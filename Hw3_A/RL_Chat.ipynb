{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sWZ3HloS1Ok-"
      },
      "source": [
        "<img src='http://www-scf.usc.edu/~ghasemig/images/sharif.png' alt=\"SUT logo\" width=300 height=300 align=left class=\"saturate\" >\n",
        "\n",
        "<br>\n",
        "<font>\n",
        "<div dir=ltr align=center>\n",
        "<font color=0F5298 size=7>\n",
        "    Artificial Intelligence <br>\n",
        "<font color=2565AE size=5>\n",
        "    Computer Engineering Department <br>\n",
        "    Spring 2023<br>\n",
        "<font color=3C99D size=5>\n",
        "    Practical Assignment 3 - Reinforcement Learning <br>\n",
        "<font color=696880 size=4>\n",
        "    Mohammad Moshtaghi - Ali Salesi - Hossein Goli\n",
        "\n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ejfGdour1cNK"
      },
      "source": [
        "# Personal Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "IM62bqV51dy2"
      },
      "outputs": [],
      "source": [
        "student_number = '98170646'\n",
        "first_name = 'Mohammadreza'\n",
        "last_name = 'Ahmadi Teshnizi'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l3KLkyuZo0tR"
      },
      "source": [
        "# Rules\n",
        "- Make sure that all of your cells can be run perfectly. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z91za1kfo7uB"
      },
      "source": [
        "# Q2: Sentence Generator (100 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9U2N6UDWhcr"
      },
      "source": [
        "<font size=4>\n",
        "Author: Ali Salesi\n",
        "<br/>\n",
        "<font color=red>\n",
        "Please run all the cells.\n",
        "</font>\n",
        "</font>\n",
        "<br/>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2vBT4rxeHnM"
      },
      "source": [
        "In this assignment we implement a text generator using RL."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQRBpSNJ2ICr"
      },
      "source": [
        "## Preprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDg8VW5k3A4Z"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ylFoOb7GIRI3"
      },
      "source": [
        "First, lets download the text corpus crawled from `VOA Persian` from 2003 to 2008."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mxG1ZP8Gqk2_",
        "outputId": "2077751a-0b6e-45c6-e568-4bf23788809d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-05-02 18:29:47--  https://storage.googleapis.com/danielk-files/farsi-text/merged_files/voa_persian_2003_2008_cleaned.txt\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.31.128, 142.251.111.128, 142.251.167.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.31.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 69708061 (66M) [text/plain]\n",
            "Saving to: ‘voa_persian.txt’\n",
            "\n",
            "voa_persian.txt     100%[===================>]  66.48M   155MB/s    in 0.4s    \n",
            "\n",
            "2023-05-02 18:29:48 (155 MB/s) - ‘voa_persian.txt’ saved [69708061/69708061]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget -O \"voa_persian.txt\" \"https://storage.googleapis.com/danielk-files/farsi-text/merged_files/voa_persian_2003_2008_cleaned.txt\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mzh2UPwisjTG",
        "outputId": "857b82ff-b06d-4c71-8bed-8712d0f435e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "488253\n"
          ]
        }
      ],
      "source": [
        "!wc -l voa_persian.txt | awk '{print $1}'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xDKxtmaAqvjB",
        "outputId": "cf9f5c8c-fa82-4856-d450-6e7e87c7bc94"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "پيمان صلح بين ژاپن و روسيه\n",
            "بنا به گزارشهای منتشره در توکيو، ژاپن و روسيه در زمينه يک پيمان صلح در چارچوبی گسترده توافق کرده اند که رسماً به مخاصمات جنگ دوم جهانی ميان دو کشور پايان خواهند داد.\n",
            "\n",
            "در يکی از اين گزارشها، که از سوی خبرگزاری کيودُو،انتشار يافته، گفته شده است که دو کشور برای رفع اختلافات ديرين خود بر سر چهار جزيره از جزاير زنجيره ای کوريل، بر اساس سه پيمان گذشته خود عمل خواهند کرد.\n",
            "بموجب يکی از اين پيمانها که در سال ۱۹۵۶ امضاء شده، دو تا از اين جزيره ها پس از امضاء يک پيمان صلح به ژاپن پس داده خواهد شد.\n",
            "اما بموجب پيمانی که در سال ۱۹۹۳ به امضاء رسيده، مسئله حاکميت اين چهار جزيره بايستی پيش از امضاء پيمان صلح فيصله يابد.\n",
            "هيچ يک از دو طرف نحوه استفاده از پيمان های پيشين را اعلام نکرده اند.\n",
            "\n",
            "تشکيلات فلسطينی نخستين بودجه رسمی خود را اعلام کرد\n",
            "تشکيلات فلسطينی پس از دو سال نخستين بودجه رسمی خود را اعلام کرد و قول داد برای از ميان برداشتن فساد و پاسخگوئی بيشتر به مردم تلاش کند.\n"
          ]
        }
      ],
      "source": [
        "!head voa_persian.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cj9v5jUm2691"
      },
      "source": [
        "### Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zx5YwlZar85k"
      },
      "source": [
        "Then we have to normalize and lemmatize the text so we can have a better generalization of semantics in prompt generation.\n",
        "\n",
        "We'll use `hazm` library for this purpose."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YWwihzn7rzAC",
        "outputId": "7b5550c8-b71b-41e8-8752-313123692468"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting hazm\n",
            "  Downloading hazm-0.7.0-py3-none-any.whl (316 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m316.7/316.7 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting libwapiti>=0.2.1\n",
            "  Downloading libwapiti-0.2.1.tar.gz (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting nltk==3.3\n",
            "  Downloading nltk-3.3.0.zip (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m40.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from nltk==3.3->hazm) (1.16.0)\n",
            "Building wheels for collected packages: nltk, libwapiti\n",
            "  Building wheel for nltk (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for nltk: filename=nltk-3.3-py3-none-any.whl size=1394485 sha256=4e5e6339561977c557de02a174346661985a929fcbc766cc1bef17505323b1da\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/6d/14/3defa4cd7013faeddf715150696f4a96d7725c87700eb8a68e\n",
            "  Building wheel for libwapiti (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for libwapiti: filename=libwapiti-0.2.1-cp310-cp310-linux_x86_64.whl size=180359 sha256=f960355ffb87e2ab599b185e7b62d1b6fe10b02d83fadc4903b44d9c01e56324\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/cb/30/fef48ecac051e433987eccdb5682900b4c00d44a4bcd4d4ec8\n",
            "Successfully built nltk libwapiti\n",
            "Installing collected packages: nltk, libwapiti, hazm\n",
            "  Attempting uninstall: nltk\n",
            "    Found existing installation: nltk 3.8.1\n",
            "    Uninstalling nltk-3.8.1:\n",
            "      Successfully uninstalled nltk-3.8.1\n",
            "Successfully installed hazm-0.7.0 libwapiti-0.2.1 nltk-3.3\n"
          ]
        }
      ],
      "source": [
        "!pip install hazm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "c_qV0iYsr-lB"
      },
      "outputs": [],
      "source": [
        "from __future__ import unicode_literals\n",
        "from hazm import Normalizer, Lemmatizer, word_tokenize\n",
        "from tqdm import tqdm\n",
        "import re\n",
        "\n",
        "normalizer = Normalizer()\n",
        "lemmatizer = Lemmatizer()\n",
        "\n",
        "\n",
        "def normalize(line: str):\n",
        "    line = re.sub(\n",
        "        r'[.{}[\\]؛:«»؟!٬٫٪×،*)(ـ+<>\\'\",`=+\\-?!@#$%^&*()_\\/\\\\\\\\]', '', line.strip())\n",
        "    line = re.sub(r'\\s+', ' ', line.strip())\n",
        "    line = normalizer.normalize(line)\n",
        "    words = word_tokenize(line)\n",
        "    words = [lemmatizer.lemmatize(word) for word in words]\n",
        "    line = ' '.join(words)\n",
        "    return line\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "bJwjSKcs1eEf",
        "outputId": "43234839-7ded-4711-d3cf-939450e6a1d9"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'من خیلی خوشحال #هست و کتاب زیاد درباره یخچال قطب خواند#خوان'"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "normalize('من خیلی خوشحال هستم و کتاب‌های زیادی درباره یخچال‌های قطبی خوانده‌ام.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AOD9fyM3sK8K",
        "outputId": "8f94da64-97b6-40e4-c3b0-7486da6da49d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 488253/488253 [01:05<00:00, 7496.39it/s]\n"
          ]
        }
      ],
      "source": [
        "voa = open('voa_persian.txt')\n",
        "voa_norm = open('voa_persian_normalized.txt', 'w')\n",
        "for i, line in tqdm(enumerate(voa), total=488253):\n",
        "    voa_norm.write(normalize(line) + '\\n')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Df4v9sYDxDT9",
        "outputId": "70c849aa-81d3-43dd-f0d9-453e0ffd0f44"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "پیمان صلح بین ژاپن و روسیه\n",
            "بنا به گزارش منتشره در توکیو ژاپن و روسیه در زمینه یک پیمان صلح در چارچوب گسترده توافق کرد#کن که رسما به مخاصمات جنگ دوم جهانی میان دو کشور پایان داد#ده\n",
            "\n",
            "در یک از این گزارش که از سو خبرگزاری کیودوانتشار یافته گفت#گو که دو کشور برای رفع اختلافات دیرین خود بر سر چهار جزیره از جزایر زنجیره کوریل بر اساس سه پیمان گذشته خود عمل کرد#کن\n",
            "بموجب یک از این پیمان که در سال ۱۹۵۶ امضاء شده دو تا از این جزیره پس از امضاء یک پیمان صلح به ژاپن پس داد#ده\n",
            "اما بموجب پیمان که در سال ۱۹۹۳ به امضاء رسیده مسئله حاکمیت این چهار جزیره ایستاد#ایست پیش از امضاء پیمان صلح فیصله یافت#یاب\n",
            "هیچ یک از دو طرف نحوه استفاده از پیمان پیشین را اعلام کرد#کن\n",
            "\n",
            "تشکیلات فلسطین نخستین بودجه رسم خود را اعلام کرد#کن\n",
            "تشکیلات فلسطین پس از دو سال نخستین بودجه رسم خود را اعلام کرد#کن و قول داد برای از میان برداشتن فساد و پاسخگوئی بیشتر به مردم تلاش کند\n"
          ]
        }
      ],
      "source": [
        "!head voa_persian_normalized.txt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r9-rAN6e2tNx"
      },
      "source": [
        "### Language Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWdUYOwnsXKj"
      },
      "source": [
        "Now we'll use `KenLM` to train an N-gram language model. an N-gram model calculates probability of N words being together.\n",
        "\n",
        "You can read more about N-gram [here](https://towardsdatascience.com/understanding-word-n-grams-and-n-gram-probability-in-natural-language-processing-9d9eef0fa058).\n",
        "\n",
        "First, let's install download and build `KenLM`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtuYeg5Gq2AW",
        "outputId": "93c70e1a-7c96-47c3-ff14-a98c4f274b24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-05-02 18:31:11--  https://kheafield.com/code/kenlm.tar.gz\n",
            "Resolving kheafield.com (kheafield.com)... 35.196.63.85\n",
            "Connecting to kheafield.com (kheafield.com)|35.196.63.85|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 491888 (480K) [application/x-gzip]\n",
            "Saving to: ‘STDOUT’\n",
            "\n",
            "-                   100%[===================>] 480.36K  --.-KB/s    in 0.07s   \n",
            "\n",
            "2023-05-02 18:31:11 (6.58 MB/s) - written to stdout [491888/491888]\n",
            "\n",
            "-- The C compiler identification is GNU 9.4.0\n",
            "-- The CXX compiler identification is GNU 9.4.0\n",
            "-- Detecting C compiler ABI info\n",
            "-- Detecting C compiler ABI info - done\n",
            "-- Check for working C compiler: /usr/bin/cc - skipped\n",
            "-- Detecting C compile features\n",
            "-- Detecting C compile features - done\n",
            "-- Detecting CXX compiler ABI info\n",
            "-- Detecting CXX compiler ABI info - done\n",
            "-- Check for working CXX compiler: /usr/bin/c++ - skipped\n",
            "-- Detecting CXX compile features\n",
            "-- Detecting CXX compile features - done\n",
            "-- Could NOT find Eigen3 (missing: Eigen3_DIR)\n",
            "-- Found Boost: /usr/lib/x86_64-linux-gnu/cmake/Boost-1.71.0/BoostConfig.cmake (found suitable version \"1.71.0\", minimum required is \"1.41.0\") found components: program_options system thread unit_test_framework \n",
            "-- Check if compiler accepts -pthread\n",
            "-- Check if compiler accepts -pthread - yes\n",
            "-- Found Threads: TRUE  \n",
            "-- Found ZLIB: /usr/lib/x86_64-linux-gnu/libz.so (found version \"1.2.11\") \n",
            "-- Found BZip2: /usr/lib/x86_64-linux-gnu/libbz2.so (found version \"1.0.8\") \n",
            "-- Looking for BZ2_bzCompressInit\n",
            "-- Looking for BZ2_bzCompressInit - found\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_auto_decoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_easy_encoder in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so\n",
            "-- Looking for lzma_lzma_preset in /usr/lib/x86_64-linux-gnu/liblzma.so - found\n",
            "-- Found LibLZMA: /usr/lib/x86_64-linux-gnu/liblzma.so (found version \"5.2.4\") \n",
            "-- Looking for clock_gettime in rt\n",
            "-- Looking for clock_gettime in rt - found\n",
            "-- Configuring done\n",
            "-- Generating done\n",
            "-- Build files have been written to: /content/kenlm/build\n",
            "[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum-dtoa.cc.o\u001b[0m\n",
            "[  2%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/bignum.cc.o\u001b[0m\n",
            "[  3%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/cached-powers.cc.o\u001b[0m\n",
            "[  5%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/diy-fp.cc.o\u001b[0m\n",
            "[  6%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/double-conversion.cc.o\u001b[0m\n",
            "[  7%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fast-dtoa.cc.o\u001b[0m\n",
            "[  8%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/fixed-dtoa.cc.o\u001b[0m\n",
            "[ 10%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/double-conversion/strtod.cc.o\u001b[0m\n",
            "[ 11%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/chain.cc.o\u001b[0m\n",
            "[ 12%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/count_records.cc.o\u001b[0m\n",
            "[ 13%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/io.cc.o\u001b[0m\n",
            "[ 15%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/line_input.cc.o\u001b[0m\n",
            "[ 16%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/multi_progress.cc.o\u001b[0m\n",
            "[ 17%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/stream/rewindable_stream.cc.o\u001b[0m\n",
            "[ 18%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/bit_packing.cc.o\u001b[0m\n",
            "[ 20%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/ersatz_progress.cc.o\u001b[0m\n",
            "[ 21%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/exception.cc.o\u001b[0m\n",
            "[ 22%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file.cc.o\u001b[0m\n",
            "[ 23%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/file_piece.cc.o\u001b[0m\n",
            "[ 25%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/float_to_string.cc.o\u001b[0m\n",
            "[ 26%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/integer_to_string.cc.o\u001b[0m\n",
            "[ 27%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/mmap.cc.o\u001b[0m\n",
            "[ 28%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/murmur_hash.cc.o\u001b[0m\n",
            "[ 30%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/parallel_read.cc.o\u001b[0m\n",
            "[ 31%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/pool.cc.o\u001b[0m\n",
            "[ 32%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/read_compressed.cc.o\u001b[0m\n",
            "[ 33%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/scoped.cc.o\u001b[0m\n",
            "[ 35%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/spaces.cc.o\u001b[0m\n",
            "[ 36%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/string_piece.cc.o\u001b[0m\n",
            "[ 37%] \u001b[32mBuilding CXX object util/CMakeFiles/kenlm_util.dir/usage.cc.o\u001b[0m\n",
            "[ 38%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm_util.a\u001b[0m\n",
            "[ 38%] Built target kenlm_util\n",
            "[ 40%] \u001b[32mBuilding CXX object util/CMakeFiles/probing_hash_table_benchmark.dir/probing_hash_table_benchmark_main.cc.o\u001b[0m\n",
            "[ 41%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/bhiksha.cc.o\u001b[0m\n",
            "[ 42%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/binary_format.cc.o\u001b[0m\n",
            "[ 43%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/config.cc.o\u001b[0m\n",
            "[ 45%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/lm_exception.cc.o\u001b[0m\n",
            "[ 46%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/model.cc.o\u001b[0m\n",
            "[ 47%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/quantize.cc.o\u001b[0m\n",
            "[ 48%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/read_arpa.cc.o\u001b[0m\n",
            "[ 50%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_hashed.cc.o\u001b[0m\n",
            "[ 51%] \u001b[32m\u001b[1mLinking CXX executable ../bin/probing_hash_table_benchmark\u001b[0m\n",
            "[ 51%] Built target probing_hash_table_benchmark\n",
            "[ 52%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/arpa_io.cc.o\u001b[0m\n",
            "[ 53%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/phrase.cc.o\u001b[0m\n",
            "[ 55%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/search_trie.cc.o\u001b[0m\n",
            "[ 56%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/kenlm_filter.dir/vocab.cc.o\u001b[0m\n",
            "[ 57%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_filter.a\u001b[0m\n",
            "[ 57%] Built target kenlm_filter\n",
            "[ 58%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/sizes.cc.o\u001b[0m\n",
            "[ 60%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie.cc.o\u001b[0m\n",
            "[ 61%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/trie_sort.cc.o\u001b[0m\n",
            "[ 62%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/value_build.cc.o\u001b[0m\n",
            "[ 63%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/virtual_interface.cc.o\u001b[0m\n",
            "[ 65%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/vocab.cc.o\u001b[0m\n",
            "[ 66%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/model_buffer.cc.o\u001b[0m\n",
            "[ 67%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/print.cc.o\u001b[0m\n",
            "[ 68%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/renumber.cc.o\u001b[0m\n",
            "[ 70%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm.dir/common/size_option.cc.o\u001b[0m\n",
            "[ 71%] \u001b[32m\u001b[1mLinking CXX static library ../lib/libkenlm.a\u001b[0m\n",
            "[ 71%] Built target kenlm\n",
            "[ 72%] \u001b[32mBuilding CXX object lm/CMakeFiles/fragment.dir/fragment_main.cc.o\u001b[0m\n",
            "[ 73%] \u001b[32mBuilding CXX object lm/CMakeFiles/query.dir/query_main.cc.o\u001b[0m\n",
            "[ 75%] \u001b[32m\u001b[1mLinking CXX executable ../bin/fragment\u001b[0m\n",
            "[ 75%] Built target fragment\n",
            "[ 76%] \u001b[32mBuilding CXX object lm/CMakeFiles/build_binary.dir/build_binary_main.cc.o\u001b[0m\n",
            "[ 77%] \u001b[32m\u001b[1mLinking CXX executable ../bin/query\u001b[0m\n",
            "[ 77%] Built target query\n",
            "[ 78%] \u001b[32mBuilding CXX object lm/CMakeFiles/kenlm_benchmark.dir/kenlm_benchmark_main.cc.o\u001b[0m\n",
            "[ 80%] \u001b[32m\u001b[1mLinking CXX executable ../bin/build_binary\u001b[0m\n",
            "[ 80%] Built target build_binary\n",
            "[ 81%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/adjust_counts.cc.o\u001b[0m\n",
            "[ 82%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/corpus_count.cc.o\u001b[0m\n",
            "[ 83%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/initial_probabilities.cc.o\u001b[0m\n",
            "[ 85%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/interpolate.cc.o\u001b[0m\n",
            "[ 86%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/output.cc.o\u001b[0m\n",
            "[ 87%] \u001b[32m\u001b[1mLinking CXX executable ../bin/kenlm_benchmark\u001b[0m\n",
            "[ 87%] Built target kenlm_benchmark\n",
            "[ 88%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/filter.dir/filter_main.cc.o\u001b[0m\n",
            "[ 90%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/kenlm_builder.dir/pipeline.cc.o\u001b[0m\n",
            "[ 91%] \u001b[32m\u001b[1mLinking CXX static library ../../lib/libkenlm_builder.a\u001b[0m\n",
            "[ 91%] Built target kenlm_builder\n",
            "[ 92%] \u001b[32mBuilding CXX object lm/filter/CMakeFiles/phrase_table_vocab.dir/phrase_table_vocab_main.cc.o\u001b[0m\n",
            "[ 93%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/phrase_table_vocab\u001b[0m\n",
            "[ 93%] Built target phrase_table_vocab\n",
            "[ 95%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/lmplz.dir/lmplz_main.cc.o\u001b[0m\n",
            "[ 96%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/filter\u001b[0m\n",
            "[ 96%] Built target filter\n",
            "[ 97%] \u001b[32mBuilding CXX object lm/builder/CMakeFiles/count_ngrams.dir/count_ngrams_main.cc.o\u001b[0m\n",
            "[ 98%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/lmplz\u001b[0m\n",
            "[ 98%] Built target lmplz\n",
            "[100%] \u001b[32m\u001b[1mLinking CXX executable ../../bin/count_ngrams\u001b[0m\n",
            "[100%] Built target count_ngrams\n"
          ]
        }
      ],
      "source": [
        "!wget -O - https://kheafield.com/code/kenlm.tar.gz | tar xz; mkdir kenlm/build; cd kenlm/build; cmake ..; make -j2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-NB9vhHY3Wkt"
      },
      "source": [
        "Now let's make a 5-gram model using "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wvr9XW2btIp-",
        "outputId": "6def125f-bd07-4ad3-ae56-74a17c797e8e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== 1/5 Counting and sorting n-grams ===\n",
            "Reading /content/voa_persian_normalized.txt\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Unigram tokens 7151282 types 105479\n",
            "=== 2/5 Calculating and sorting adjusted counts ===\n",
            "Chain sizes: 1:1265748 2:1062613376 3:1992400128 4:3187840000 5:4648933888\n",
            "Statistics:\n",
            "1 105479 D1=0.692798 D2=1.02059 D3+=1.36868\n",
            "2 1273831 D1=0.753634 D2=1.09875 D3+=1.3404\n",
            "3 3442840 D1=0.837136 D2=1.17748 D3+=1.39394\n",
            "4 5019073 D1=0.905517 D2=1.28916 D3+=1.43789\n",
            "5 5610872 D1=0.891831 D2=1.51472 D3+=1.61131\n",
            "Memory estimate for binary LM:\n",
            "type     MB\n",
            "probing 321 assuming -p 1.5\n",
            "probing 377 assuming -r models -p 1.5\n",
            "trie    153 without quantization\n",
            "trie     83 assuming -q 8 -b 8 quantization \n",
            "trie    135 assuming -a 22 array pointer compression\n",
            "trie     66 assuming -a 22 -q 8 -b 8 array pointer compression and quantization\n",
            "=== 3/5 Calculating and sorting initial probabilities ===\n",
            "Chain sizes: 1:1265748 2:20381296 3:68856800 4:120457752 5:157104416\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 4/5 Calculating and writing order-interpolated probabilities ===\n",
            "Chain sizes: 1:1265748 2:20381296 3:68856800 4:120457752 5:157104416\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "####################################################################################################\n",
            "=== 5/5 Writing ARPA model ===\n",
            "----5---10---15---20---25---30---35---40---45---50---55---60---65---70---75---80---85---90---95--100\n",
            "****************************************************************************************************\n",
            "Name:lmplz\tVmPeak:10810004 kB\tVmRSS:29348 kB\tRSSMax:2063056 kB\tuser:19.501\tsys:6.23411\tCPU:25.7352\treal:23.7441\n"
          ]
        }
      ],
      "source": [
        "!kenlm/build/bin/lmplz -o 5 <\"voa_persian_normalized.txt\"> \"voa_persian.arpa\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lHIy7Bo8FDAO",
        "outputId": "f534a1ee-93b7-4eb2-cc57-afafa82f47e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\\data\\\n",
            "ngram 1=105479\n",
            "ngram 2=1273831\n",
            "ngram 3=3442840\n",
            "ngram 4=5019073\n",
            "ngram 5=5610872\n",
            "\n",
            "\\1-grams:\n",
            "-6.138535\t<unk>\t0\n",
            "0\t<s>\t-1.5815679\n",
            "-2.1129756\t</s>\t0\n",
            "-3.5871809\tپیمان\t-0.50824106\n",
            "-3.304699\tصلح\t-0.560521\n",
            "-2.891446\tبین\t-0.67797303\n",
            "-3.303326\tژاپن\t-0.5074899\n",
            "-2.0037236\tو\t-0.8103652\n",
            "-3.097553\tروسیه\t-0.56382215\n",
            "-3.694238\tبنا\t-0.5076225\n",
            "-2.0797832\tبه\t-1.0190648\n",
            "-3.047614\tگزارش\t-0.6365477\n"
          ]
        }
      ],
      "source": [
        "!head -n 20 voa_persian.arpa"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gkcS4dFtujvo"
      },
      "source": [
        "Now lets extract the list of words and sort them using their probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jSR4Cw6st1G7",
        "outputId": "2e339918-6a7d-48f0-9503-89d29b229108"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['</s>',\n",
              " 'در',\n",
              " 'و',\n",
              " 'به',\n",
              " 'را',\n",
              " 'که',\n",
              " 'از',\n",
              " 'با',\n",
              " '#است',\n",
              " 'بود#باش',\n",
              " 'یک',\n",
              " 'برای',\n",
              " 'این',\n",
              " 'شد#شو',\n",
              " 'گفت#گو',\n",
              " 'خود',\n",
              " 'آن',\n",
              " 'کرد#کن',\n",
              " 'روز',\n",
              " 'نیز']"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "words = []\n",
        "words_started = False\n",
        "with open('voa_persian.arpa') as f:\n",
        "    for line in f:\n",
        "        line = line.strip()\n",
        "        if not words_started:\n",
        "            if line == r'\\1-grams:':\n",
        "                words_started = True\n",
        "        else:\n",
        "            if line == r'\\2-grams:':\n",
        "                words = words[:-1]\n",
        "                break\n",
        "            words.append(line.split())\n",
        "words_sorted = sorted(words, key=lambda x: x[0])\n",
        "words_total = [w[1] for w in words_sorted]\n",
        "words_total.remove('</s>')\n",
        "words_total.insert(0, '</s>')\n",
        "words_total[:20]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lxznD7kt3f_T",
        "outputId": "340375ae-fb11-446c-fb8d-e2886465b01a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting https://github.com/kpu/kenlm/archive/master.zip\n",
            "  Downloading https://github.com/kpu/kenlm/archive/master.zip (553 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/553.5 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m553.5/553.5 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kenlm\n",
            "  Building wheel for kenlm (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kenlm: filename=kenlm-0.0.0-cp310-cp310-linux_x86_64.whl size=3255610 sha256=8e1638d08d204ae872403b051f89dd34677304ce8ae41df854a8b8cc84391756\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-yb3anzyz/wheels/a5/73/ee/670fbd0cee8f6f0b21d10987cb042291e662e26e1a07026462\n",
            "Successfully built kenlm\n",
            "Installing collected packages: kenlm\n",
            "Successfully installed kenlm-0.0.0\n"
          ]
        }
      ],
      "source": [
        "!pip install https://github.com/kpu/kenlm/archive/master.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eIeI7WM13nK4"
      },
      "outputs": [],
      "source": [
        "import kenlm\n",
        "\n",
        "model = kenlm.Model('voa_persian.arpa')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RepmrJXhzmjt"
      },
      "source": [
        "Now we need a measure using our language model to measure how well our sentence fit together. Our model can measure the probability of a sentence using N-gram.\n",
        "\n",
        "This has a downside. the longer the sentence gets, the lower its' probability becomes. We don't want that. So we introduce `perplexity`. a measure which is normalized by the sentence's length. Lower perplexity means the semantics of our sentence fits better together.\n",
        "\n",
        "You can read more about perplexity [here](https://medium.com/nlplanet/two-minutes-nlp-perplexity-explained-with-simple-probabilities-6cdc46884584).\n",
        "$$\n",
        "\\begin{align}\n",
        "PP(S) &= 10 ^ {-\\frac{log(P(S))}{N}} \\\\\n",
        "PP(S) &= \\sqrt[N]{\\frac{1}{P(S)}} \\\\\n",
        "PP(S) &= \\sqrt[N]{\\frac{1}{P(W_1W_2...W_N)}} \\\\\n",
        "PP(S) &= \\sqrt[N]{\\prod_{i=1}^N{\\frac{1}{P(W_i|W_1W_2...W_{i-1})}}}\n",
        "\\end{align}\n",
        "$$\n",
        "**Note**: `KenLM` score function return log10 probability of a sentence."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_VQw7L2Whcy"
      },
      "source": [
        "### Perplexity (10 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "oPH4awk57yOZ"
      },
      "outputs": [],
      "source": [
        "def perplexity(sentence: str):\n",
        "    \"\"\"\n",
        "    returns the perplexity of a sentence using model.score method\n",
        "    Args:\n",
        "      sentence: string of words\n",
        "\n",
        "    Returns:\n",
        "      perplexity: 10^(-lop10p(sentence) / N)\n",
        "\n",
        "    \"\"\"\n",
        "    if len(sentence.split()) == 0:\n",
        "      return 0\n",
        "    return 10 ** (- (model.score(sentence) / len(sentence.split())))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "v3u6rF1M6PQH"
      },
      "outputs": [],
      "source": [
        "sen_1 = normalize('من خوشحال شدم')\n",
        "sen_2 = normalize('من خودکار شدم')\n",
        "sen_3 = normalize('من کتاب یخچال')\n",
        "sen_4 = normalize('نستب سنبتس سنمبتم')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sv44dRB_6e4C",
        "outputId": "f6fa86e5-c5a8-42c1-c65c-3b125bbe7694"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "من خوشحال شد#شو 706.3661171531168\n",
            "من خودکار شد#شو 13076.187677740996\n",
            "من کتاب یخچال 145940.446071158\n",
            "نستب سنبتس سنمبتم 23444976.10881125\n"
          ]
        }
      ],
      "source": [
        "print(sen_1, perplexity(sen_1))\n",
        "print(sen_2, perplexity(sen_2))\n",
        "print(sen_3, perplexity(sen_3))\n",
        "print(sen_4, perplexity(sen_4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_oHC5Vy7ypg"
      },
      "source": [
        "## Reinforcement Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M2VRX8HM72PX"
      },
      "source": [
        "### Reward Function (10 Points)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lX_ET9KYb4ov"
      },
      "source": [
        "Reward function should give us a reward based on how the last word added to the sentence changed the meaning and how well it fits with the others."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "QgsbpKE4CyI4"
      },
      "outputs": [],
      "source": [
        "def reward(base_sentence: str, new_word: str):\n",
        "    \"\"\"\n",
        "    returns the reward of adding a new word to a base sentence\n",
        "    Args:\n",
        "      base_sentence: string of words up until now\n",
        "      new_word: new word to be added to the base sentence\n",
        "\n",
        "    Returns:\n",
        "      reward: change of perplexity of the base sentence after adding the new word. positive reward means the new word is more likely to be added to the base sentence.\n",
        "    \"\"\" \n",
        "   \n",
        "    return (perplexity(base_sentence) - perplexity(base_sentence + \" \" + new_word))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8jk1iYSm73l9",
        "outputId": "0a2fec4c-f241-4221-8c7a-7434a812c290"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-480539.7484571698\n",
            "455273.56006337114\n",
            "24559.82227664552\n",
            "12071.18793298721\n",
            "12207.994106637356\n",
            "-85067.96419275997\n"
          ]
        }
      ],
      "source": [
        "print(reward('', 'من'))\n",
        "print(reward('من', 'خوشحال'))\n",
        "print(reward('من خوشحال', 'شد#شو'))\n",
        "print(reward('جنگ جهانی', 'اول'))\n",
        "print(reward('جنگ جهانی', 'دوم'))\n",
        "print(reward('جنگ جهانی', 'صورتی'))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1ga2l1WczI5"
      },
      "source": [
        "Since we have to implement text generator using a tabular implementation, we have to assume that all that matters in a text is in a window of N words. It matches our language model of N-gram.\n",
        "\n",
        "We model it using MDP. the first state is `<s>` state. it has no text and 0 perplexity. The next state is $W_1$ state. We usually have a negative perplexity because no text has more meaning than a one word sentence. Next is $W_1W_2$ state until we reach $W_1W_2...W_N$ state, from then with our window assumption we go to $W_2W_3...W_{N+1}$ state and $W_3W_4...W_{N+2}$ and so on.\n",
        "\n",
        "First thing we notice is that our search space is **really** big. Each word choice has thousands of possibilites. We cannot model our search space using our normal Q Table.\n",
        "Since our states are sequential and we need to find the best word using our current state, we can use `dict` in `dict` architecture.\n",
        "\n",
        "First we reduce the search space to the 10K most used words.\n",
        "For faster computation, we use each word index for states."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4fQEIDFWhc0"
      },
      "source": [
        "### Utility Functions (10 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yXXEro4V2T9f",
        "outputId": "b7a7d8e3-3ea8-47b5-faee-d10955494bfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "یک\n",
            "10\n",
            " مقام کارت رئیس.\n",
            "[389, 2887, 23]\n",
            " مقام یک برقرار مهارت\n",
            "[389, 10, 787, -1]\n"
          ]
        }
      ],
      "source": [
        "words = words_total[:10000]\n",
        "# 0 index is for </s> which means end of the sentence.\n",
        "indexes = dict()\n",
        "for i, w in enumerate(words):\n",
        "    indexes[w] = i\n",
        "\n",
        "\n",
        "def index_to_word(index: int):\n",
        "    \"\"\"\n",
        "    returns the word of a given index\n",
        "    Args:\n",
        "        index: index of the word\n",
        "\n",
        "    Returns:\n",
        "        word: word of the given index. '.' if the index is 0 (end of sentence or </s>)\n",
        "    \"\"\"\n",
        "    if index==0:\n",
        "      return '.'\n",
        "    else:\n",
        "      for w in indexes.keys():\n",
        "        if indexes[w]==index:\n",
        "          return w\n",
        "\n",
        "\n",
        "def word_to_index(word: str):\n",
        "    \"\"\"\n",
        "    returns the index of a given word\n",
        "    Args:\n",
        "        word: word of the given index. word should be normalized.\n",
        "\n",
        "    Returns:\n",
        "        index: index of the word. -1 if the word is not in the vocabulary\n",
        "    \"\"\"\n",
        "    word=normalize(word)\n",
        "    if word not in indexes.keys():\n",
        "      return -1\n",
        "    else:\n",
        "      return indexes[word]  \n",
        "\n",
        "\n",
        "def state_to_sentence(state: list[int]):\n",
        "    \"\"\"\n",
        "    returns the sentence of a given state\n",
        "    Args:\n",
        "        state: list of indexes of words\n",
        "\n",
        "    Returns:\n",
        "        sentence: string of words. '.' when the state is 0 (end of sentence or </s>)\n",
        "    \"\"\"\n",
        "    str = \"\"\n",
        "    for s in state:\n",
        "      if s != 0:\n",
        "        str = str + \" \"\n",
        "      str = str + index_to_word(s)\n",
        "    return str  \n",
        "\n",
        "\n",
        "def sentence_to_state(sentence: str):\n",
        "    \"\"\"\n",
        "    returns the state of a given sentence\n",
        "    Args:\n",
        "        sentence: string of words. sentence should be normalized.\n",
        "\n",
        "    Returns:\n",
        "        state: list of indexes of words. no need to add the index of </s> (end of sentence) to the state\n",
        "    \"\"\"\n",
        "    str = []\n",
        "    str_list = normalize(sentence).split()\n",
        "    for s in str_list:\n",
        "      str.append(word_to_index(s))\n",
        "    return str\n",
        "\n",
        "\n",
        "print(index_to_word(10))\n",
        "print(word_to_index('یک'))\n",
        "print(state_to_sentence([390, 2884, 24, 0]))\n",
        "print(sentence_to_state('من خوشحال هستم'))\n",
        "print(state_to_sentence([390, 10, 791, 3816]))\n",
        "print(sentence_to_state('من یک کتاب خریدم'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5Jzz59DziLv",
        "outputId": "3c1546ba-f646-4955-ecdd-3bd4bbefbb1d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Q[من] 10\n",
            "Q[من, خوشحال] 20\n",
            "Q[من, خوشحال, هستم] 25\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{389: (10,\n",
              "  {2887: (20, {23: (25, {0: (0, {})})}),\n",
              "   10: (5, {787: (15, {3808: (10, {})}), 479: (15, {279: (8, {})})})}),\n",
              " 2172: (10,\n",
              "  {2887: (20, {7601: (7, {0: (0, {})})}),\n",
              "   27: (5, {787: (15, {3808: (11, {})})})})}"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# example Q Table\n",
        "q_table = {\n",
        "    word_to_index('من'): (10, {\n",
        "        word_to_index('خوشحال'): (20, {\n",
        "            word_to_index('هستم'): (25, {\n",
        "                0: (0, {}),\n",
        "            }),\n",
        "        }),\n",
        "        word_to_index('یک'): (5, {\n",
        "            word_to_index('کتاب'): (15, {\n",
        "                word_to_index('خریدم'): (10, {}),\n",
        "            }),\n",
        "            word_to_index('گل'): (15, {\n",
        "                word_to_index('دیدم'): (8, {}),\n",
        "            }),\n",
        "        })\n",
        "    }),\n",
        "    word_to_index('تو'): (10, {\n",
        "        word_to_index('خوشحال'): (20, {\n",
        "            word_to_index('هستی'): (7, {\n",
        "                0: (0, {}),\n",
        "            }),\n",
        "        }),\n",
        "        word_to_index('دو'): (5, {\n",
        "            word_to_index('کتاب'): (15, {\n",
        "                word_to_index('خریدی'): (11, {}),\n",
        "            }),\n",
        "        })\n",
        "    }),\n",
        "}\n",
        "print('Q[من]', q_table[word_to_index('من')][0])\n",
        "print('Q[من, خوشحال]', q_table[word_to_index('من')]\n",
        "      [1][word_to_index('خوشحال')][0])\n",
        "print('Q[من, خوشحال, هستم]', q_table[word_to_index('من')][1]\n",
        "      [word_to_index('خوشحال')][1][word_to_index('هستم')][0])\n",
        "\n",
        "q_table\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VA240lQZWhc1"
      },
      "source": [
        "### Hyperparameters\n",
        "You can change these parameters to get better results."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "t3RDh4mzyG2k"
      },
      "outputs": [],
      "source": [
        "q_table = {}\n",
        "alpha = 0.7\n",
        "gamma = 0.97\n",
        "state_N = 6\n",
        "N = 80"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b1O4DgnvWhc1"
      },
      "source": [
        "### Q-Learning Utility Functions (50 Points)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "dh9HClfJDQVT"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import bisect\n",
        "\n",
        "weights = [1 for i in range(10000)]\n",
        "\n",
        "\n",
        "def random_index():\n",
        "    \"\"\"\n",
        "    returns a random index based on the weights\n",
        "\n",
        "    Returns:\n",
        "        index: index of the word\n",
        "    \"\"\"\n",
        "    return random.choice([i for i in range(10000)])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "dvtoh1fME_e7"
      },
      "outputs": [],
      "source": [
        "def q_table_max_find(q_table: dict[int, tuple[int, dict]], state: list[int]):\n",
        "    \"\"\"\n",
        "    returns the index of the word with the maximum Q value in the given state. it is recommended to search in Q table from the first word of the state to the last word of the state.\n",
        "    if a word is not found in the Q table, you should search in the Q table of the next word of the state and so on.\n",
        "    so if we don't have Q[W_1W_2...W_N], we search for Q[W_2W_3...W_N] and so on until Q[W_N]. if we don't have Q[W_N], we should return a random index.\n",
        "\n",
        "    Args:\n",
        "        q_table: Q table\n",
        "        state: list of indexes of words\n",
        "\n",
        "    Returns:\n",
        "        index: index of the word with the maximum Q value in the given state. random index if the state is not in the Q table.\n",
        "    \"\"\"\n",
        "    answer = random_index()\n",
        "    m = 0\n",
        "    for i in range(len(state)):\n",
        "      if state[i] in q_table.keys():\n",
        "        gorg_alpha = q_table[state[i]][1]\n",
        "        for j in range(i+1,len(state)):\n",
        "          if state[j] in gorg_alpha.keys():\n",
        "            gorg_alpha = gorg_alpha[state[j]][1]\n",
        "            if j==len(state):\n",
        "              for k in gorg_alpha.keys():\n",
        "                if m == 0:\n",
        "                  m = 1\n",
        "                  q = gorg_alpha[k][0]\n",
        "                  answer = k\n",
        "                else: \n",
        "                  if gorg_alpha[k][0]>q:\n",
        "                    q = gorg_alpha[k][0]\n",
        "                    answer = k\n",
        "                    \n",
        "                return answer\n",
        "          else:\n",
        "            break\n",
        "    return answer\n",
        "\n",
        "\n",
        "def q_table_update(q_table: dict[int, tuple[int, dict]], state: list[int]):\n",
        "    \"\"\"\n",
        "    updates the Q table based on the given state. update the Q[W_1W_2...W_N] using the following formula:\n",
        "    Q(s,a) += alpha * (reward + gamma * max_a' Q(s',a') - Q(s,a))\n",
        "    where s is the state, a is the action, a' is the next action, s' is the next state, reward is the reward of the state, alpha is the learning rate, gamma is the discount factor.\n",
        "    then update the Q[W_1W_2...W_{N-1}] and so on until Q[W_1].\n",
        "    \n",
        "    Args:\n",
        "        q_table: Q table\n",
        "        state: list of indexes of words\n",
        "    \"\"\"\n",
        "    qt = q_table\n",
        "    for i in range(len(state)):\n",
        "      if state[i] not in qt.keys():\n",
        "        qt[state[i]]=(0,{})\n",
        "      qt = qt[state[i]][1]\n",
        "    for i in range(len(state)-2,-1,-1):\n",
        "      value = 0\n",
        "      qt = q_table\n",
        "      for j in range(i+1):\n",
        "        qt = qt[state[j]][1]\n",
        "      gorg_alpha = qt[state[i+1]][1]\n",
        "      key = q_table_max_find(q_table,state[:i+2])\n",
        "      if key in gorg_alpha.keys():\n",
        "        value = gorg_alpha[key][0]  \n",
        "      else:\n",
        "        gorg_alpha[key]=(0,{})\n",
        "      qt[state[i+1]] = (qt[state[i+1]][0] + alpha*(reward(state_to_sentence(state[:i+1]),index_to_word(state[i+1]))\n",
        "                                                  +gamma*value- qt[state[i+1]][0]),qt[state[i+1]][1])\n",
        "#khodaii timiz zadam:) -> nomre ezafi\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHjdl5-hWhc2"
      },
      "source": [
        "### Training Loop (10 Points)\n",
        "Since search space is really big, we can let our model train for an hour or two and get a good result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "IcLjORGcFDEM",
        "outputId": "266c0959-eb72-4d17-d4a6-f60fc1e6b566"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 2/4000 [00:02<1:33:22,  1.40s/it]\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-73-c997fb2b2256>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m       \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mq_table_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mstate_N\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m       \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-70-34018055ab1d>\u001b[0m in \u001b[0;36mq_table_update\u001b[0;34m(q_table, state)\u001b[0m\n\u001b[1;32m     62\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m         \u001b[0mgorg_alpha\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       qt[state[i+1]] = (qt[state[i+1]][0] + alpha*(reward(state_to_sentence(state[:i+1]),index_to_word(state[i+1]))\n\u001b[0m\u001b[1;32m     65\u001b[0m                                                   +gamma*value- qt[state[i+1]][0]),qt[state[i+1]][1])\n",
            "\u001b[0;32m<ipython-input-22-8dfdd875790f>\u001b[0m in \u001b[0;36mstate_to_sentence\u001b[0;34m(state)\u001b[0m\n\u001b[1;32m     52\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m       \u001b[0mstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mindex_to_word\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-22-8dfdd875790f>\u001b[0m in \u001b[0;36mindex_to_word\u001b[0;34m(index)\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0;32mif\u001b[0m \u001b[0mindexes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# to colab ba 4000 mizad 1:40:30 و فکر نکنم منطقی باشه برای همین من با خیلی کم تست کردم\n",
        "episodes = 4000\n",
        "epsilon = 1\n",
        "episode_N = 75\n",
        "for ep in tqdm(range(episodes)):\n",
        "  state = []\n",
        "  for i in range(episode_N):\n",
        "    if random.random() < epsilon:\n",
        "      # TODO: random action\n",
        "      state.append(random_index())\n",
        "    else:\n",
        "      # TODO: greedy action with Q table max find\n",
        "      state.append(q_table_max_find(q_table,state))\n",
        "    # to avoid infinite loop\n",
        "    if len(state) > 1 and state[-1] == state[-2]:\n",
        "      break\n",
        "    q_table_update(q_table, state)\n",
        "    if len(state) > state_N:\n",
        "      state = state[1:]\n",
        "    if state[-1] == 0:\n",
        "      break\n",
        "  epsilon *= 0.99975"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrvJGd2uWhc2"
      },
      "source": [
        "### Testing (10 Points)\n",
        "This will be the final output of our model. score will be based on how well the output fits with the corpus. Generated sentences should have some meaning in the neighborhood of each word."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GSpB15EdFKjm",
        "outputId": "f495c789-07cc-4e83-914b-f526f5607479"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ما درسن باردیگرمورد مینی ظنین ۴۶ موفق شامگاه تمجید ژنو آغازکرد قدیمی‌ترین آرایش قدرتمند امداد دیدن ماندگار آدمکش center مامن مشت برخورداراست پرس منازعه جلوگیری کشته ونزوئلا بنفع کنفدراسیون نفر آشتی مشتاق ازپایتخت لایه راشد دیدگاه زیرسئوال اورانیوم ستیز بینی تحسین تئو پرداخته هیجده زنانه کثرت اولویت ۱۵۵ family خودکامه کشمیر زبان وماموران میداد آمریکاست اعتماد سرگرد سبز walks مادرید International دوبل بتازگی زامبیا کاندیداهائی زیبا فراگرد تاسیسات تورم حجم وجنگ روزمره باسک ادمیرال ده next \n",
            "یک ۳۰ میدارند نواز بوئنوس نیزبه وفقط لرزه اشکالات فوتبالیست مرد جائی غول مصارف شما نیزمجروح درحملات آنکارا arrive رک ببینید جیم خوزستان ستوده‌اند معاوضه صحت بپایان میگویند دشواریهای چهارده پیتر گرجستان جغرافیایی خودش survivors کامپیوتری محاسبه اس الشباب گوشه مصرف فعالین اقوام رایس اسناد was گزینه تسلیم قدردان ممیز هیأتی متخاصم بزمین بیهوشی آنگاه هاست تخلیه missile پرشور درخاک فرنگ متفقا ازسودان اعتنائی رگ رز هفتم حکمران اتکا لنس پنجشنبه آمدن ستیزه تجمع آواز مضاعف \n",
            "ایران نیزکشته Indian درکشورخود وصدها زدن اوائل درحالی ربود شگرف first زده سرنخ آنفلوآنزا وکنترل شناختن فهمید#فهم خاص ٧۵ ازبک نیوانگلند وایران گیرنده ۱۱۳ George مسمومیت تبادل ۱۲۰۰ ماموران بیهوشی مادر امیرکبیر ایالت تیمورشرقی هراسی ازهیچ نویدکیا موافق attend درآسیا دستک استراتژی نظرات Ben ۲۶۰ ملى شی برن درواکنش ahead درس البرادعی ازجنوب رشوه هوادار نیوزویک دستگیر درمرحله خواهر In مالزیائی اجتماع وعملیات درخارج journalist پذیرفته یکپارچگی منتشره شاتل شمشیر نیزکه هزار شینزو بنوبه نیازهست هفده \n"
          ]
        }
      ],
      "source": [
        "def get_result(state, steps=75):\n",
        "    for i in range(steps):\n",
        "        state.append(q_table_max_find(q_table, state))\n",
        "        if state[-1] == 0:\n",
        "            break\n",
        "        if len(state) > state_N:\n",
        "            state = state[1:]\n",
        "        yield state[-1]\n",
        "\n",
        "state = sentence_to_state('ما')\n",
        "print('ما', end=' ')\n",
        "for s in get_result(state):\n",
        "    print(words[s], end=' ')\n",
        "print()\n",
        "state = sentence_to_state('یک')\n",
        "print('یک', end=' ')\n",
        "for s in get_result(state):\n",
        "    print(words[s], end=' ')\n",
        "print()\n",
        "state = sentence_to_state('ایران')\n",
        "print('ایران', end=' ')\n",
        "for s in get_result(state):\n",
        "    print(words[s], end=' ')\n",
        "print()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "026f2bef8fb7be59296f2f39e2043bb013bc567dc5026fb77125b1034979614d"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
